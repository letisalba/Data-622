---
title: "Data 622 - Homework 1"
author: "Leticia Salazar"
date: "October 8, 2023"
output: 
  html_document:
    theme: sandstone
    highlight: haddock
    toc: yes
    toc_float: yes
  pdf_document:
    dev: cairo_pdf
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
$~$

#### Pre-work
1. Visit the following website and explore the range of sizes of this dataset (from 100 to 5 million records):
https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/ or
(new) https://www.kaggle.com/datasets
2. Select 2 files to download
Based on your computer's capabilities (memory, CPU), select 2 files you can handle (recommended one small, one large)
3. Download the files
4. Review the structure and content of the tables, and think about the data sets (structure, size, dependencies, labels, etc)
5. Consider the similarities and differences in the two data sets you have downloaded
6. Think about how to analyze and predict an outcome based on the datasets available
7. Based on the data you have, think which two machine learning algorithms presented so far could be used to analyze the data

#### Deliverable
1. Essay (minimum 500 word document) Write a short essay explaining your selection of algorithms and how they relate to the data and what you are trying to do
2. Exploratory Analysis using R or Python (submit code + errors + analysis as notebook or copy/paste to document)
Explore how to analyze and predict an outcome based on the data available. This will be an exploratory exercise, so feel free to show errors and warnings that raise during the analysis. Test the code with both datasets selected and compare the results.


$~$

#### Load Libraries:
Below are the libraries used to complete this assignment
```{r, warning=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE}
library(tidyverse)
library(skimr)
#install.packages('rpart.plot') # must install if not already
library(rpart) # decision tree package
library(rpart.plot) # decision tree display package
library(lemon)
library(knitr)
```

$~$

### The Data:
The data chosen from Excel BI Analytics were the 100 sales records for the small and 5000 sales records for the large. The data sets are included in my [GitHub](https://github.com/letisalba/Data-622/tree/master/Homework-1/csv) and read into R.
```{r, loading data, echo = FALSE}
small_df <- read.csv("https://raw.githubusercontent.com/letisalba/Data-622/master/Homework-1/csv/100%20Sales%20Records.csv")
large_df <- read.csv("https://raw.githubusercontent.com/letisalba/Data-622/master/Homework-1/csv/5000%20Sales%20Records.csv")
```

**The small data set:**
```{r, echo=FALSE, kable.opts=list(caption="data frame is now printed using `kable`.")}

kable(head(small_df),caption='Normal `kable` usage.')
#head(small_df)
```

$~$

**The large data set:**
```{r, echo=FALSE}
kable(head(large_df),caption='Normal `kable` usage.')
#head(large_df)
```

$~$

### Data Exploration:

Let's explore the data sets; first the `small_df` data set, using the `skimr` library we can obtain quick summary statistics beyond the `summary()`. We notice that we have 14 variables split into 7 character and 7 numeric. There seems to be no missing values, so this will have a simple preparation before we build out model.
```{r, echo=FALSE}
skim(small_df)
```

Now, the `large_df` dataset is composed of 5000 values of the same 14 variables as the `small` data set. It also has 7 character and 7 numeric variables with no missing values.
```{r, echo=FALSE}
skim(large_df)
```

$~$

### Data Preparation:

```{r}

```

$~$

### Model Building:

```{r}

```

$~$

### Conclusion:

#### Answer questions such as:
1. Are the columns of your data correlated?
2. Are there labels in your data? Did that impact your choice of algorithm?
3. What are the pros and cons of each algorithm you selected?
4. How your choice of algorithm relates to the datasets (was your choice of algorithm impacted by the datasets you chose)?
5. Which result will you trust if you need to make a business decision?
6. Do you think an analysis could be prone to errors when using too much data, or when using the least amount possible?
7. How does the analysis between data sets compare?


































