---
title: "Data 622 - Homework 1"
author: "Leticia Salazar"
date: "October 8, 2023"
output: 
  html_document:
    theme: sandstone
    highlight: haddock
    toc: yes
    toc_float: yes
  pdf_document:
    dev: cairo_pdf
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
$~$

#### Pre-work
1. Visit the following website and explore the range of sizes of this dataset (from 100 to 5 million records):
https://excelbianalytics.com/wp/downloads-18-sample-csv-files-data-sets-for-testing-sales/ or
(new) https://www.kaggle.com/datasets
2. Select 2 files to download
Based on your computer's capabilities (memory, CPU), select 2 files you can handle (recommended one small, one large)
3. Download the files
4. Review the structure and content of the tables, and think about the data sets (structure, size, dependencies, labels, etc)
5. Consider the similarities and differences in the two data sets you have downloaded
6. Think about how to analyze and predict an outcome based on the datasets available
7. Based on the data you have, think which two machine learning algorithms presented so far could be used to analyze the data

#### Deliverable
1. Essay (minimum 500 word document) Write a short essay explaining your selection of algorithms and how they relate to the data and what you are trying to do
2. Exploratory Analysis using R or Python (submit code + errors + analysis as notebook or copy/paste to document)
Explore how to analyze and predict an outcome based on the data available. This will be an exploratory exercise, so feel free to show errors and warnings that raise during the analysis. Test the code with both datasets selected and compare the results.


$~$

#### Load Libraries:
Below are the libraries used to complete this assignment
```{r, warning=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE}
#library(tidyverse)
library(skimr)
#install.packages('rpart.plot') # must install if not already
library(rpart) # decision tree package
library(rpart.plot) # decision tree display package
library(lemon)
library(knitr)
```

$~$

### The Data:
The data chosen from Excel BI Analytics were the 100 sales records for the small and 5000 sales records for the large. The data sets are included in my [GitHub](https://github.com/letisalba/Data-622/tree/master/Homework-1/csv) and read into R.
```{r, loading data, echo = FALSE}
small_df <- read.csv("https://raw.githubusercontent.com/letisalba/Data-622/master/Homework-1/csv/100%20Sales%20Records.csv")
large_df <- read.csv("https://raw.githubusercontent.com/letisalba/Data-622/master/Homework-1/csv/5000%20Sales%20Records.csv")
```

**The small data set:**
```{r, echo=FALSE, kable.opts=list(caption="data frame is now printed using `kable`.")}

kable(head(small_df),caption='Normal `kable` usage.')
#head(small_df)
```

$~$

**The large data set:**
```{r, echo=FALSE}
kable(head(large_df),caption='Normal `kable` usage.')
#head(large_df)
```

$~$

### Data Exploration:

Let's explore the data sets; first the `small_df` data set, using the `skimr` library we can obtain quick summary statistics beyond the `summary()`. We notice that we have 14 variables split into 7 character and 7 numeric. There seems to be no missing values, so this will have a simple preparation before we build out model.
```{r, echo=FALSE}
skim(small_df)
```

Now, the `large_df` dataset is composed of 5000 values of the same 14 variables as the `small` data set. It also has 7 character and 7 numeric variables with no missing values.
```{r, echo=FALSE}
skim(large_df)
```

```{r}
# Load the library
library(tidymodels)
library(tidyr)
```


$~$

### Data Preparation:

Before we build our model, I first converted some columns into `as.factor` and the two columns containing dates `as.Date`. Below are the results.
```{r, echo=FALSE}
# small data set 
small_df$Region <- as.factor(small_df$Region)
small_df$Country <- as.factor(small_df$Country)
small_df$Item.Type <- as.factor(small_df$Item.Type)
small_df$Sales.Channel <- as.factor(small_df$Sales.Channel)
small_df$Order.Priority <- as.factor(small_df$Order.Priority)
small_df$Order.Date <- as.Date(small_df$Order.Date, "%m/%d/%Y")
small_df$Ship.Date <- as.Date(small_df$Ship.Date, "%m/%d/%Y")

# large data set
large_df$Region <- as.factor(large_df$Region)
large_df$Country <- as.factor(large_df$Country)
large_df$Item.Type <- as.factor(large_df$Item.Type)
large_df$Sales.Channel <- as.factor(large_df$Sales.Channel)
large_df$Order.Priority <- as.factor(large_df$Order.Priority)
large_df$Order.Date <- as.Date(large_df$Order.Date, "%m/%d/%Y")
large_df$Ship.Date <- as.Date(large_df$Ship.Date, "%m/%d/%Y")
```

**Small dataset:**
```{r, echo=FALSE}
kable(head(small_df),caption='Normal `kable` usage.')
```

$~$

**Large dataset:**
```{r, echo=FALSE}
kable(head(large_df),caption='Normal `kable` usage.')
```

```{r}
# # Prepare the dataset for ggplot2
# small_df <- small_df %>%
#   pivot_longer(cols = c(Region, Country, Item.Type, Sales.Channel, Order.Priority, Order.Date, Order.ID, Ship.Date, Units.Sold, Unit.Price, Unit.Cost, Total.Revenue, Total.Cost, Total.Profit), 
#                names_to = "Names", 
#                values_to = "Values")
# 
#  #pivot_longer(cols = c(Region, Country, Item.Type, Sales.Channel, Order.Priority, Order.Date, Order.ID, Ship.Date, Units.Sold, Unit.Price, Unit.Cost, Total.Revenue, Total.Cost, Total.Profit)#everything(),
#               #names_to = "variable",
#               #values_to = "value")
# 
# # Create a histogram for all numeric variables in one plot
# small_df_histograms <- ggplot(small_df, aes(x = value)) +
#  geom_histogram(bins = 30, color = "black", fill = "lightblue") +
#  facet_wrap(~variable, scales = "free", ncol = 4) +
#  labs(title = "Histograms of Small Dataset",
#       x = "Value",
#       y = "Frequency") +
#  theme_minimal()
# 
# # Plot the histograms
# print(small_df_histograms)
```
```{r}
# Split the data into training and testing sets

set.seed(123)

# small data set
small_df_split <- initial_split(small_df, prop = 0.75)
small_train <- training(small_df_split)
small_test <- testing(small_df_split)

# large data set
large_df_split <- initial_split(large_df, prop = 0.75)
large_train <- training(large_df_split)
large_test <- testing(large_df_split)
```


$~$

### Model Building:

I've selected to create decision trees for both datasets and predict the order

```{r}
# Create a decision tree model specification for small data set
tree_spec_small <- decision_tree() %>%
 set_engine("rpart") %>%
 set_mode("regression")

# Fit the model to the training data
tree_fit_small <- tree_spec_small %>%
 fit(Units.Sold ~ ., data = small_train)

# Create a decision tree model specification for large data set
tree_spec_large <- decision_tree() %>%
 set_engine("rpart") %>%
 set_mode("regression")

# Fit the model to the training data
tree_fit_large <- tree_spec_large %>%
 fit(Units.Sold ~ ., data = large_train)
```


```{r}
# Make predictions on the testing data for the small data set
small_predictions <- tree_fit_small %>%
 predict(small_test) %>%
 pull(.pred)

# Calculate RMSE and R-squared for the small data set
small_metrics <- metric_set(rmse, rsq)
small_model_performance <- small_test %>%
 mutate(small_predictions = small_predictions) %>%
 small_metrics(truth = Units.Sold, estimate = small_predictions)

print(small_model_performance)

# Make predictions on the testing data for the large data set
large_predictions <- tree_fit_large %>%
 predict(large_test) %>%
 pull(.pred)

# Calculate RMSE and R-squared for the large data set
large_metrics <- metric_set(rmse, rsq)
large_model_performance <- large_test %>%
 mutate(large_predictions = large_predictions) %>%
 large_metrics(truth = Units.Sold, estimate = large_predictions)

print(large_model_performance)
```

```{r}
# Plot the decision tree for small data set
rpart.plot(tree_fit_small$fit, type = 4, extra = 101, under = TRUE, cex = 0.8, box.palette = "auto")
```


```{r}
# Plot the decision tree for large data set
rpart.plot(tree_fit_large$fit, type = 4, extra = 101, under = TRUE, cex = 0.8, box.palette = "auto")
```

```{r}
rules_small <- rpart.rules(tree_fit_small$fit)
print(rules_small)
```

```{r}
rules_large <- rpart.rules(tree_fit_large$fit)
print(rules_large)
```


$~$

### Conclusion:

#### Answer questions such as:
1. Are the columns of your data correlated?
2. Are there labels in your data? Did that impact your choice of algorithm?
3. What are the pros and cons of each algorithm you selected?
4. How your choice of algorithm relates to the datasets (was your choice of algorithm impacted by the datasets you chose)?
5. Which result will you trust if you need to make a business decision?
6. Do you think an analysis could be prone to errors when using too much data, or when using the least amount possible?
7. How does the analysis between data sets compare?


































